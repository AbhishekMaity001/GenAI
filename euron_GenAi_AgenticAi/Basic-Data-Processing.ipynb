{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8b553ba-61ed-4ca4-8b40-38cbcf5fd0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "562fd061-8d99-409a-8996-bc50a3ab0cc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"book\")  # all means download all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94b9b312-9fab-4da6-9c60-948b0018fa69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_eng is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_rus is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package bcp47 to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package english_wordnet to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package english_wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker_tab is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger_tab is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package mock_corpus to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mock_corpus is already up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pe08 is already up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt_tab to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt_tab is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets_json to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets_json is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2022 to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "408d69ae-2eac-4113-a804-f213277669ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Abhishek\n",
      "[nltk_data]     Maity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.book import *\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "624f6a1d-04bf-4a67-a013-7f03504ad929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "666a9a70-c78e-413f-bc2f-8c246ce49d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Text: Moby Dick by Herman Melville 1851>,\n",
       " <Text: Sense and Sensibility by Jane Austen 1811>,\n",
       " <Text: The Book of Genesis>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1, text2, text3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eab4376d-9ce5-4231-871a-cf3c83a0b786",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "222f9410-9071-4240-817c-962f8d062021",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"Hi my name is Abhishek Maity, and I am learning generative AI with agentic ai from euron , by Sudhanshu sir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "295ada85-992c-47c9-8c7f-545ad699d057",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'my',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Abhishek',\n",
       " 'Maity',\n",
       " ',',\n",
       " 'and',\n",
       " 'I',\n",
       " 'am',\n",
       " 'learning',\n",
       " 'generative',\n",
       " 'AI',\n",
       " 'with',\n",
       " 'agentic',\n",
       " 'ai',\n",
       " 'from',\n",
       " 'euron',\n",
       " ',',\n",
       " 'by',\n",
       " 'Sudhanshu',\n",
       " 'sir']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac8566ac-6f7d-43b8-b453-017cfec5e16a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi my name is Abhishek Maity, and I am learning generative AI with agentic ai from euron , by Sudhanshu sir']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0264720f-0ab4-4b4b-8ddd-11af1d042bc7",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12179e6b-46b1-44cb-aa93-350177538733",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02d8d2ea-40dd-4e93-afd0-13615f47cf7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    }
   ],
   "source": [
    "stopwords = stopwords.words(\"english\")\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c16033b-212b-410d-8d06-36201dfcfbc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41a8626f-d426-4a56-a5cd-afd3d76dffff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'my', 'name', 'is', 'abhishek', 'maity', ',', 'and', 'i', 'am', 'learning', 'generative', 'ai', 'with', 'agentic', 'ai', 'from', 'euron', ',', 'by', 'sudhanshu', 'sir']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(txt.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8af7cb7-bcda-4746-9b0c-71c64e325824",
   "metadata": {},
   "source": [
    "### Cleaning of stopwords from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c73b84e1-cd97-4fbf-8858-4847fc728258",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi',\n",
       " 'name',\n",
       " 'abhishek',\n",
       " 'maity',\n",
       " ',',\n",
       " 'learning',\n",
       " 'generative',\n",
       " 'ai',\n",
       " 'agentic',\n",
       " 'ai',\n",
       " 'euron',\n",
       " ',',\n",
       " 'sudhanshu',\n",
       " 'sir']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in word_tokenize(txt.lower()) if x.lower() not in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189f6cbf-8add-4bb7-9966-d8a5d2aa6069",
   "metadata": {},
   "source": [
    "### Stemming & Lemmatization (some context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2a69a26-f657-420b-851e-462fc943c0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer, LancasterStemmer, SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f13a1eb4-6745-43fb-a1f1-a1f4091a21c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PorterStemmer>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming = PorterStemmer()\n",
    "stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c27673a9-9597-4805-85aa-4efec5f00526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'name', 'Abhishek', 'Maity', ',', 'learning', 'generative', 'AI', 'agentic', 'ai', 'euron', ',', 'Sudhanshu', 'sir']\n"
     ]
    }
   ],
   "source": [
    "txt_cleaned = [x for x in word_tokenize(txt) if x.lower() not in stopwords]\n",
    "print(txt_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9daefed9-8e0b-495a-859d-c81db11f641b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi',\n",
       " 'name',\n",
       " 'abhishek',\n",
       " 'maiti',\n",
       " ',',\n",
       " 'learn',\n",
       " 'gener',\n",
       " 'ai',\n",
       " 'agent',\n",
       " 'ai',\n",
       " 'euron',\n",
       " ',',\n",
       " 'sudhanshu',\n",
       " 'sir']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[stemming.stem(x) for x in txt_cleaned]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e01a989-9e70-4122-99af-b4199bbebb95",
   "metadata": {},
   "source": [
    "### POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "821efc7a-6412-47c1-b437-00727cd50f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3cc00d8f-e21f-469e-9ae2-1a6c3cd0f1f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('apple', 'NN'),\n",
       " ('abhishek maity', 'NN'),\n",
       " ('running', 'VBG'),\n",
       " ('dancing', 'VBG')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag([\"apple\", \"abhishek maity\", \"running\", \"dancing\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6cbdf6-741e-463c-aa38-aad45caaa07d",
   "metadata": {},
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42fc446f-cda9-4b51-90c1-35f2871c0264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i love nlp', 'i learn gen ai', 'i am working with accenture']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\"i love nlp\", \"i learn gen ai\", \"i am working with accenture\"]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0df8353f-c8d8-42c9-8d2b-620d49de56e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['am',\n",
       " 'nlp',\n",
       " 'accenture',\n",
       " 'ai',\n",
       " 'love',\n",
       " 'learn',\n",
       " 'gen',\n",
       " 'i',\n",
       " 'with',\n",
       " 'working']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words = list(set(\" \".join(corpus).split()))\n",
    "unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97b587c6-29e7-4d68-8305-49460b1b720b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'am': 0,\n",
       " 'nlp': 1,\n",
       " 'accenture': 2,\n",
       " 'ai': 3,\n",
       " 'love': 4,\n",
       " 'learn': 5,\n",
       " 'gen': 6,\n",
       " 'i': 7,\n",
       " 'with': 8,\n",
       " 'working': 9}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index = {v:k for k,v in dict(enumerate(unique_words)).items()}\n",
    "word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "40ff30b5-1149-4e8f-8c16-f43040257ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]]\n"
     ]
    }
   ],
   "source": [
    "one_hot_encoder = []\n",
    "for sentence in corpus:\n",
    "    # print(sentence.split())\n",
    "    sentence_vector = []\n",
    "    for word in sentence.split():\n",
    "        vector = [0] * len(unique_words)\n",
    "        vector[word_to_index[word]] = 1\n",
    "        # print(vector)\n",
    "        sentence_vector.append(vector)\n",
    "    # print(sentence_vector)\n",
    "    \n",
    "    one_hot_encoder.append(sentence_vector)\n",
    "print(one_hot_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc8b212-5137-4724-b43e-9c691d078eed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf23e6a7-4771-4839-9956-a569bb726954",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "74b48226-9865-42d2-b4d7-f9ffd590a333",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(vocabulary=unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e924c5f-86ac-4f2c-831b-5426fe86db40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i love nlp', 'i learn gen ai nlp nlp nlp', 'i am working with accenture']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\"i love nlp\", \"i learn gen ai nlp nlp nlp\", \"i am working with accenture\"]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c3317e8a-34a9-4d1d-a8a1-26d3f4bd760e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i love nlp', 'i learn gen ai nlp nlp nlp', 'i am working with accenture']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "034f093a-07bc-4c88-bb4d-6bf8636ab8af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
       "\twith 10 stored elements and shape (3, 10)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = vectorizer.fit_transform(corpus)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2425ebbb-ffaf-4811-995e-44c80c99f761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 3, 0, 1, 0, 1, 1, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 0, 0, 0, 0, 1, 1]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c3d8b07-e5d0-47ce-a682-05cef7222a4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['am', 'nlp', 'accenture', 'ai', 'love', 'learn', 'gen', 'i',\n",
       "       'with', 'working'], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "99a99534-88b3-4b52-9fcf-57bf2cd5802a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.718281828459045"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.exp(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d54ec0-a70c-4b0b-b0b5-b145a310cf47",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "af8f0d40-235d-4aa5-8a81-489dc6142a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3d113f5b-33a0-41ac-b1c0-5cdd0919cd0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 10 stored elements and shape (3, 9)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = vectorizer.fit_transform(corpus)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "150ce996-deec-4ed2-8f62-9834410c8099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.79596054, 0.60534851, 0.        , 0.        ],\n",
       "       [0.        , 0.34909607, 0.        , 0.34909607, 0.34909607,\n",
       "        0.        , 0.79648968, 0.        , 0.        ],\n",
       "       [0.5       , 0.        , 0.5       , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.5       , 0.5       ]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b7bd3b-0ed8-4588-915a-e04094c95b9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "516420fe-7a4d-455f-9d22-63828d1aa4e2",
   "metadata": {},
   "source": [
    "## Word 2 Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "050c5394-3c5c-41e6-acfc-2a71f6de715f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aeb81ae2-dc51-40a5-8429-f83156e56e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "419b8a7f-1e03-472e-877e-857a6cadb190",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"MY Name is Abhishek Maity\",\n",
    "    \"I aM having 7 years of experience in data science\",\n",
    "    \"sadf lkdsf sdf sdf #$%^ *&@ \\\\sdfalj \"\n",
    "    \"nlp is very very intresting\",\n",
    "    \"we are trying to learn word2vec\",\n",
    "    \"till now i have learned stemming, stopwords, lemmatization, one hot encoding, tfidf, bag of words\",\n",
    "    \"this word2vec is going to perform better than older methods\",\n",
    "    \"bag of words is frequency based method\",\n",
    "    \"word2vec is a small neural network model which was developed by google back in 2013\",\n",
    "    \"We will first clean the data in data processing techniques, then will create a model\",\n",
    "    \"CBOW and skipgram are 2 models which comes under word2vec\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9185a2bf-a369-45ff-bedf-d918f47106ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'euron nlp 123 .'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"Euron NLP 123 .\".lower()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cfcedf07-16fe-4adb-9137-00d1d7bfa7c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'euron nlp  .'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.sub(r'\\d+', \"\", s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f5d5bca5-8285-41d2-9733-f1eb6a31b2dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "79eed155-dce7-4979-b54d-3c0c31ac7238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'euron nlp 123 '"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.translate(str.maketrans(\"\", \"\", string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ad6a5586-da8e-4f4c-bf4f-74d19a30a2c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['euron', 'nlp', '123', '.']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5702ea6b-518e-4ae9-b7f9-47ffe0229e83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['euron', 'nlp', '123', '.']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4b953840-5cb8-4b55-b68f-616af9845329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['euron', 'nlp', '123', '.']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in word_tokenize(s) if x not in stopwords.words(\"english\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4032af4d-cabd-40f2-93c4-1a9ac0f8fc5e",
   "metadata": {},
   "source": [
    "### Text Pre processing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8cc4b950-fcc7-4284-b418-1c2154ce9a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def word_preprocessing(text):\n",
    "    text = text.lower()  # lower case\n",
    "    text = re.sub(r'\\d+', \"\", text) # remove special character\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    word = word_tokenize(text)\n",
    "    word = [x for x in word if x not in stopwords.words(\"english\")]\n",
    "    return word\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c0211a5a-1930-44a9-a81e-2796166d2e80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MY Name is Abhishek Maity',\n",
       " 'I aM having 7 years of experience in data science',\n",
       " 'sadf lkdsf sdf sdf #$%^ *&@ \\\\sdfalj nlp is very very intresting',\n",
       " 'we are trying to learn word2vec',\n",
       " 'till now i have learned stemming, stopwords, lemmatization, one hot encoding, tfidf, bag of words',\n",
       " 'this word2vec is going to perform better than older methods',\n",
       " 'bag of words is frequency based method',\n",
       " 'word2vec is a small neural network model which was developed by google back in 2013',\n",
       " 'We will first clean the data in data processing techniques, then will create a model',\n",
       " 'CBOW and skipgram are 2 models which comes under word2vec']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ba1f6b95-ab1d-48bc-bc6c-583d68c72a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['name', 'abhishek', 'maity'],\n",
       " ['years', 'experience', 'data', 'science'],\n",
       " ['sadf', 'lkdsf', 'sdf', 'sdf', 'sdfalj', 'nlp', 'intresting'],\n",
       " ['trying', 'learn', 'wordvec'],\n",
       " ['till',\n",
       "  'learned',\n",
       "  'stemming',\n",
       "  'stopwords',\n",
       "  'lemmatization',\n",
       "  'one',\n",
       "  'hot',\n",
       "  'encoding',\n",
       "  'tfidf',\n",
       "  'bag',\n",
       "  'words'],\n",
       " ['wordvec', 'going', 'perform', 'better', 'older', 'methods'],\n",
       " ['bag', 'words', 'frequency', 'based', 'method'],\n",
       " ['wordvec',\n",
       "  'small',\n",
       "  'neural',\n",
       "  'network',\n",
       "  'model',\n",
       "  'developed',\n",
       "  'google',\n",
       "  'back'],\n",
       " ['first',\n",
       "  'clean',\n",
       "  'data',\n",
       "  'data',\n",
       "  'processing',\n",
       "  'techniques',\n",
       "  'create',\n",
       "  'model'],\n",
       " ['cbow', 'skipgram', 'models', 'comes', 'wordvec']]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_corpus = [word_preprocessing(text) for text in corpus]\n",
    "# [text for text in corpus]\n",
    "processed_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20541de9-3cd8-4b96-9c3a-e3b992885ab7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd183a16-b527-4fb8-abb7-08edab629a43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6e3359-c95f-4fe9-a578-1e81f9856ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8ddacb-0eef-4676-a793-22d6fd1cb3b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cba531-2ec3-4ee3-954c-296f96c775a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
